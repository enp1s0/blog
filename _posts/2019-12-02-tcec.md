---
layout: post
title:  "精度補正を加えたTensorコアによる単精度積について"
date:   2019-12-02 01:08:41 +0900
categories: cuda
---

{% include header.markdown %}

<a href="https://adventar.org/calendars/10896">消えたCUDA関連の旧ブログ記事を復元するひとり Advent Calendar 2024</a>の記事です。

<b>追記</b> : 新たに精度補正に関する研究で論文を出しました。NVIDIA A100上でTensorコアを用いることで、FP32 SIMTコアを用いた場合と同じ精度で、かつFP32の理論ピーク性能以上でSGEMMをエミュレートする研究です。<br> Hiroyuki Ootomo, Rio Yokota, <a href='https://arxiv.org/abs/2203.03341'>Recovering single precision accuracy from Tensor Cores while surpassing the FP32 theoretical peak performance</a>, IJHPCA, 2022
<hr>
CUDA Advent Calendar 2019 2日目の記事です．<br>
自分の修論の内容です．

<h2 id='a'>Tensorコアで単精度行列積を計算したい場合、精度補正がなぜ必要なのか?</h2>
<p>
Tensorコアへの積計算を行いたい行列はFP16で入力する必要があります．<br>
このためTensorコアがいくら内部でFP32計算を行おうとも計算精度の劣化は免れません．<br>
これほ精度の劣化を修正しようというのが今回の記事の内容です．<br>
実は既存研究もあったりします．<br>
<a href='https://arxiv.org/abs/1803.04014'>Stefano Markidis, Steven Wei Der Chien, Erwin Laure, Ivy Bo Peng, Jeffrey S. Vetter - <b>NVIDIA Tensor Core Programmability, Performance & Precision</b> arXiv:1803.04014</a><br>
あと，自分の研究でも彼らとはちょっと違いますが使っています．（彼らの手法は無駄があると考えています）<br>
<a href='https://static.momo86.net/f/1/sc19-tsqr-on-tc-poster'>Hiroyuki Ootomo, Rio Yokota - <b>TSQR on TensorCores</b> SC19 Research Poster</a>
</p>

<h2>どうやって精度を補正するの?</h2>
<img src="{{site.baseurl}}/assets/images/tcec.png">
<p>
自分の手法ではFP32からFP16に型を落とす際に発生する誤差を別変数でとっておき，これを用いて精度の修正を行っていきます．<br>
この手法では行列積の回数は3倍となり，加えて修正項の計算が入るため計算量は増えます．<br>
しかしそもそもTensorコアの計算速度が速いことなどを考えると（チューニング次第では）いい感じに高速に計算できます．
<p>
もっとも，完全にFP32の精度が出るわけではありません．<br>
FP32の仮数部は23bit+ケチ1bitで24bitなのに対しFP16は10bit+1bitで11bitなため，FP16変数2つではFP32の仮数部をすべて表せません．<br>
また，Tensorコア内部の足しこみがFP32なのでFP64行列積の精度修正は難しいです．<br>
指数部長の違いもスケールすれば解決する場合としない場合があります．
</p>

<h2 id='c'>おわり</h2>
<p>
今後のアーキでTensorコアの精度面で何かしらの進歩があると楽しそうですね．
</p>
